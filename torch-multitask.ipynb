{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install timm\n! pip install torchsummary\n#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null 2>&1\n#!python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev > /dev/null 2>&1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T08:28:53.922965Z","iopub.execute_input":"2021-07-20T08:28:53.923354Z","iopub.status.idle":"2021-07-20T08:29:09.396693Z","shell.execute_reply.started":"2021-07-20T08:28:53.923261Z","shell.execute_reply":"2021-07-20T08:29:09.395712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch \nimport torchvision\nfrom timm import models\nfrom torch import nn\nimport timm\nfrom torchsummary import summary\nimport torch.nn.functional as F\nimport gc\nimport albumentations as A\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nimport cv2 as cv\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom collections import defaultdict\nfrom torch.nn.parallel.data_parallel import data_parallel","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:09.399551Z","iopub.execute_input":"2021-07-20T08:29:09.399825Z","iopub.status.idle":"2021-07-20T08:29:13.451058Z","shell.execute_reply.started":"2021-07-20T08:29:09.399797Z","shell.execute_reply":"2021-07-20T08:29:13.450035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_models = timm.list_models(pretrained=True)\n#list(x for x in all_models if 'efficientnet' in x)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.453066Z","iopub.execute_input":"2021-07-20T08:29:13.453345Z","iopub.status.idle":"2021-07-20T08:29:13.459184Z","shell.execute_reply.started":"2021-07-20T08:29:13.453319Z","shell.execute_reply":"2021-07-20T08:29:13.457845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiTask(nn.Module):\n    def __init__(self, model_name='efficientnet_b5', num_classes=4, pretrained=True):\n        super(MultiTask, self).__init__()\n        base_model = timm.create_model(model_name, pretrained=pretrained, in_chans=3)\n        self.bl_0 = nn.Sequential(\n            base_model.conv_stem,\n            base_model.bn1,\n            base_model.act1\n        )\n        self.bl_1 = base_model.blocks[0]\n        self.bl_2 = base_model.blocks[1]\n        self.bl_3 = base_model.blocks[2]\n        self.bl_4 = base_model.blocks[3]\n        self.bl_5 = base_model.blocks[4]\n        self.bl_6 = base_model.blocks[5]\n        self.bl_7 = base_model.blocks[6]\n        self.bl_8 = nn.Sequential(\n            base_model.conv_head,\n            base_model.bn2,\n            base_model.act2\n        )\n        \n        self.logit = nn.Linear(2560, num_classes)\n        self.mask = nn.Sequential(\n            nn.Conv2d(224, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )\n        \n    \n    def forward(self, x):\n        batch_size = len(x)\n        x = self.bl_0(x)\n        x = self.bl_1(x)\n        x = self.bl_2(x)\n        x = self.bl_3(x)\n        x = self.bl_4(x)\n        x = self.bl_5(x)\n        \n        mask = self.mask(x) # [batch_size, 1, 38, 38]\n        #print(mask.shape)\n        \n        x = self.bl_6(x)\n        x = self.bl_7(x)\n        x = self.bl_8(x)\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        \n        logit = self.logit(x)\n        \n        return logit, mask\n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.461166Z","iopub.execute_input":"2021-07-20T08:29:13.461656Z","iopub.status.idle":"2021-07-20T08:29:13.47684Z","shell.execute_reply.started":"2021-07-20T08:29:13.461621Z","shell.execute_reply":"2021-07-20T08:29:13.475958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    augm = A.Compose([A.augmentations.transforms.HorizontalFlip(p=0.5),\n                  A.augmentations.transforms.VerticalFlip(p=0.5),\n                  A.augmentations.geometric.rotate.Rotate(p=0.5),\n                  A.OneOf([\n                       A.augmentations.transforms.Blur(),\n                       A.augmentations.transforms.GlassBlur(),\n                       A.augmentations.transforms.GaussianBlur(),\n                       A.augmentations.transforms.GaussNoise(),\n                       A.augmentations.transforms.RandomGamma(),\n                       A.augmentations.transforms.InvertImg(),\n                       #A.augmentations.transforms.RandomFog()\n                   ], p=0.5)])\n    train_path = '../input/covid19-detection-890pxpng-study/train/' \n    test_path = '../input/covid19-detection-890pxpng-study/test/'\n    mask_path = '../input/covid19-detection-890pxpng-study/ROI Mask/'\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:32:08.528538Z","iopub.execute_input":"2021-07-20T08:32:08.528929Z","iopub.status.idle":"2021-07-20T08:32:08.539215Z","shell.execute_reply.started":"2021-07-20T08:32:08.528899Z","shell.execute_reply":"2021-07-20T08:32:08.538211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_process(image_id, mask=False, train=True):\n    path = Config.mask_path if mask else Config.train_path\n    image_path = path + image_id + '.png'\n    if mask:\n        image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n        if image is None:\n            path = Config.train_path + image_id + '.png'\n            image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n        image = cv.resize(image, (38, 38))\n        \n    else:\n        image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n        image = cv.resize(image, (600, 600))\n    if train:\n        image = Config.augm(image=image)['image']\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.490284Z","iopub.execute_input":"2021-07-20T08:29:13.490659Z","iopub.status.idle":"2021-07-20T08:29:13.501085Z","shell.execute_reply.started":"2021-07-20T08:29:13.490624Z","shell.execute_reply":"2021-07-20T08:29:13.500048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CreateDataset(Dataset):\n    def __init__(self, df, train=True):\n        super(CreateDataset, self).__init__()\n        self.df = df\n        self.label_cols = df.columns[4:8]\n        self.train = train\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, indx):\n        image_id = df.loc[indx, 'id']\n        label = df.loc[indx, self.label_cols].values\n        \n        image = image_process(image_id, train=self.train) / 255.\n        mask = image_process(image_id, mask=True, train=self.train) / 255.\n        \n        output = {'image': image,\n                  'mask': mask,\n                  'label': label}\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.502528Z","iopub.execute_input":"2021-07-20T08:29:13.502919Z","iopub.status.idle":"2021-07-20T08:29:13.511619Z","shell.execute_reply.started":"2021-07-20T08:29:13.502881Z","shell.execute_reply":"2021-07-20T08:29:13.510303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(batch):\n    collate = defaultdict(list)\n    \n    for i in batch:\n        for k, v in i.items():\n            collate[k].append(v)\n            \n    batch_size = len(batch)\n    label = np.ascontiguousarray(np.stack(collate['label'])).astype(np.float32)\n    collate['label'] = torch.from_numpy(label)\n    \n    image = np.stack(collate['image'])\n    image = image.reshape(batch_size, 1, 600, 600).repeat(3,1)\n    image = np.ascontiguousarray(image)\n    collate['image'] = torch.from_numpy(image)\n    \n    mask = np.stack(collate['mask'])\n    mask = mask.reshape(batch_size, 1, 38, 38)#.repeat(3, 1)\n    mask = np.ascontiguousarray(mask)\n    collate['mask'] = torch.from_numpy(mask)\n    \n    return collate   ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.515265Z","iopub.execute_input":"2021-07-20T08:29:13.515968Z","iopub.status.idle":"2021-07-20T08:29:13.525945Z","shell.execute_reply.started":"2021-07-20T08:29:13.515923Z","shell.execute_reply":"2021-07-20T08:29:13.524916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/covid19-detection-890pxpng-study/train.csv')\ndf['fold'] = -1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.527886Z","iopub.execute_input":"2021-07-20T08:29:13.528633Z","iopub.status.idle":"2021-07-20T08:29:13.605402Z","shell.execute_reply.started":"2021-07-20T08:29:13.528591Z","shell.execute_reply":"2021-07-20T08:29:13.60459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groupkfold = GroupKFold(n_splits=5)\nfor fold, (train_indx, valid_indx) in enumerate(groupkfold.split(df, groups=df.id.tolist())):\n    df.loc[valid_indx, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.608522Z","iopub.execute_input":"2021-07-20T08:29:13.608865Z","iopub.status.idle":"2021-07-20T08:29:13.657063Z","shell.execute_reply.started":"2021-07-20T08:29:13.60884Z","shell.execute_reply":"2021-07-20T08:29:13.656116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_valid(net, valid_loader):\n\n    valid_probability = []\n    valid_truth = []\n    valid_num = 0\n\n    net.eval()\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        batch_size = len(batch['index'])\n        image = batch['image'].cuda()\n        onehot = batch['onehot']\n        label = onehot.argmax(-1)\n\n        with torch.no_grad():\n                logit, mask = data_parallel(net,image)\n                probability = F.softmax(logit,-1)\n\n        valid_num += batch_size\n        valid_probability.append(probability.data.cpu().numpy())\n        valid_truth.append(label.data.cpu().numpy())\n        #print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n\n    truth = np.concatenate(valid_truth)\n    probability = np.concatenate(valid_probability)\n    predict = probability.argsort(-1)[::-1]\n\n    loss = np_loss_cross_entropy(probability,truth)\n    topk = (predict==truth.reshape(-1,1))\n    acc  = topk[:, 0]\n    topk = topk.mean(0).cumsum()\n    acc = [acc[truth==i].mean() for i in range(num_study_label)]\n\n    return [loss, topk[0], topk[1]]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:29:13.658353Z","iopub.execute_input":"2021-07-20T08:29:13.658738Z","iopub.status.idle":"2021-07-20T08:29:13.668538Z","shell.execute_reply.started":"2021-07-20T08:29:13.658692Z","shell.execute_reply":"2021-07-20T08:29:13.66737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    valid_df = df[df.fold == i]\n    train_df = df[df.fold != i]\n    \n    train_dataset = CreateDataset(train_df, train=True)\n    valid_dataset = CreateDataset(valid_df, train=False)\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n    \n    train_loader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=8,\n        drop_last=True,\n        num_workers=4,\n        pin_memory=True,\n        worker_init_fn=lambda id_: np.random.seed(torch.initial_seed() // 2 ** 32 + id_),\n        collate_fn=collate\n                             )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler=valid_sampler,\n        batch_size=16,\n        drop_last=False,\n        num_workers=4,\n        pin_memory=True,\n        collate_fn=collate\n                             )\n    \n    \n    net = MultiTask().cuda()\n    \n    valid_loss = np.zeros(4,np.float32)\n    train_loss = np.zeros(3,np.float32)\n    batch_loss = np.zeros_like(train_loss)\n    sum_train_loss = np.zeros_like(train_loss)\n    sum_train = 0\n    loss0 = torch.FloatTensor([0]).cuda().sum()\n    loss1 = torch.FloatTensor([0]).cuda().sum()\n    loss2 = torch.FloatTensor([0]).cuda().sum()\n    \n    iteration = 0\n    epoch = 0\n    rate = 0\n    while  iteration < 20:\n\n        for t, batch in enumerate(train_loader):\n\n            '''if iteration in iter_save:\n                if iteration != start_iteration:\n                    torch.save({\n                        'state_dict': net.state_dict(),\n                        'iteration': iteration,\n                        'epoch': epoch,\n                    }, out_dir + '/checkpoint/%08d_model.pth' % (iteration))\n                    pass\n\n            if (iteration % iter_valid == 0):\n                    valid_loss = do_valid(net, valid_loader)  #\n                    pass\n\n            if (iteration % iter_log == 0):\n                print('\\r', end='', flush=True)\n                log.write(message(mode='log') + '\\n')'''\n\n\n            def get_learning_rate(optimizer):\n                lr=[]\n                for param_group in optimizer.param_groups:\n                    lr +=[ param_group['lr'] ]\n                lr = lr[0]\n\n                return lr\n            \n            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()))\n            rate = get_learning_rate(optimizer)\n\n            batch_size = 8\n            image = batch['image'].cuda()\n            truth_mask = batch['mask'].cuda()\n            #truth_mask = F.interpolate(truth_mask, size=(38,38), mode='bilinear', align_corners=False)\n            onehot = batch['label'].cuda()\n            label = onehot.argmax(-1)\n\n            net.train()\n            optimizer.zero_grad()\n\n        \n            print('fp32')\n            logit, mask = data_parallel(net, image)\n            loss0 = F.cross_entropy(logit, label)\n            loss1 = F.binary_cross_entropy_with_logits(mask, truth_mask)\n\n            (loss0 + loss1).backward()\n            optimizer.step()\n\n            epoch += 1 / len(train_loader)\n            iteration += 1\n\n            batch_loss = np.array([loss0.item(), loss1.item(), loss2.item()])\n            sum_train_loss += batch_loss\n            sum_train += 1\n            if iteration % 100 == 0:\n                train_loss = sum_train_loss / (sum_train + 1e-12)\n                sum_train_loss[...] = 0\n                sum_train = 0\n\n            print('\\r', end='', flush=True)\n            print(message(mode='print'), end='', flush=True)\n\n\n        #log.write('\\n')\n\n    \n   ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T08:35:12.93994Z","iopub.execute_input":"2021-07-20T08:35:12.940378Z","iopub.status.idle":"2021-07-20T08:35:16.141435Z","shell.execute_reply.started":"2021-07-20T08:35:12.94034Z","shell.execute_reply":"2021-07-20T08:35:16.139053Z"},"trusted":true},"execution_count":null,"outputs":[]}]}